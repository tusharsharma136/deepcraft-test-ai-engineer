# -*- coding: utf-8 -*-
"""AI_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hd7V4Z3P4cCNgg0kMi3XKQCSJ942GI2s
"""

import pandas as pd
data= pd.read_csv("ett.csv")

data.head()

#Check for missing data
data.isnull().sum()

data['date'] = pd.to_datetime(data['date'])
data.set_index('date', inplace=True)
data['OT'].plot(figsize=(10, 6), title='Oil Temperature Over Time')

"""#EDA

Understanding Data Trends (Initial Visualizations)
"""

data['OT_MA_7'] = data['OT'].rolling(window=7).mean()
data['OT_MA_30'] = data['OT'].rolling(window=30).mean()
data[['OT', 'OT_MA_7', 'OT_MA_30']].plot(figsize=(10,6), title='Oil Temperature with Moving Averages')

#Decompose the time series to separate out trends, seasonality, and noise.
from statsmodels.tsa.seasonal import seasonal_decompose
decomposition = seasonal_decompose(data['OT'], model='additive', period=24) # assuming hourly data
decomposition.plot()

"""#Data Integrity Check (Missing Values, Outliers)"""

#Check for missing values
data.isnull().sum()

#Missing values exist, visualizing gaps in the time series.
import missingno as msno
msno.matrix(data)

#Visualize outliers in oil temperature using a boxplot
import seaborn as sns
sns.boxplot(x=data['OT'])

"""Seasonal and Temporal Features"""

data['hour'] = data.index.hour
data['day'] = data.index.day
data['weekday'] = data.index.weekday
data['month'] = data.index.month

#Analyze how OT behaves with respect to these features using boxplots or line plots
sns.boxplot(x='hour', y='OT', data=data)

sns.boxplot(x='weekday', y='OT', data=data)

"""Correlation Analysis (Feature Relationships)"""

corr_matrix = data.corr()
sns.heatmap(corr_matrix, annot=True)

"""Stationarity Check"""

#Conduct an Augmented Dickey-Fuller (ADF) Test to check for stationarity
from statsmodels.tsa.stattools import adfuller
result = adfuller(data['OT'])
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')

""" Lag Features (Time Series Dependency)"""

data['OT_lag1'] = data['OT'].shift(1)
data['OT_lag2'] = data['OT'].shift(2)
# Drop missing values after lagging
data = data.dropna()

data.head()

""" Seasonal Decomposition & Fourier Transform (Advanced Analysis)"""

from numpy.fft import fft
import matplotlib.pyplot as plt
import numpy as np
fft_series = fft(data['OT'])
plt.plot(np.abs(fft_series))

"""#Data processing and feature engineering

Handle Missing Data
"""

# Check for missing values in the dataset
missing_values = data.isnull().sum()
print(missing_values)

"""Feature Engineering"""

data.head()

import numpy as np

# Creating cyclical seasonal features for hour and month
data['sin_hour'] = np.sin(2 * np.pi * data['hour'] / 24)
data['cos_hour'] = np.cos(2 * np.pi * data['hour'] / 24)
data['sin_month'] = np.sin(2 * np.pi * data['month'] / 12)
data['cos_month'] = np.cos(2 * np.pi * data['month'] / 12)

# Additional lag features (lags of 24 hours and 168 hours (1 day and 1 week))
data['OT_lag24'] = data['OT'].shift(24)  # 1 day lag
data['OT_lag168'] = data['OT'].shift(168)  # 1 week lag

# Rolling mean and standard deviation over different windows
data['OT_MA_3'] = data['OT'].rolling(window=3).mean()  # 3-hour rolling mean
data['OT_STD_3'] = data['OT'].rolling(window=3).std()  # 3-hour rolling std
data['OT_MA_24'] = data['OT'].rolling(window=24).mean()  # 1-day rolling mean
data['OT_STD_24'] = data['OT'].rolling(window=24).std()  # 1-day rolling std

# Interaction between hour and sensor readings
data['HUFL_hour_interaction'] = data['HUFL'] * data['hour']
data['MUFL_month_interaction'] = data['MUFL'] * data['month']

from sklearn.preprocessing import StandardScaler

# Scaling continuous columns (excluding cyclical features)
scaler = StandardScaler()
columns_to_scale = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT',
                    'OT_MA_7', 'OT_MA_30', 'OT_lag1', 'OT_lag2', 'OT_lag24',
                    'OT_lag168', 'OT_MA_3', 'OT_STD_3', 'OT_MA_24', 'OT_STD_24']
data[columns_to_scale] = scaler.fit_transform(data[columns_to_scale])

# Forward fill for any missing values
data = data.fillna(method='ffill')

# Alternatively, interpolation
data['OT'] = data['OT'].interpolate(method='linear')

"""Feature selection"""

# Correlation matrix to see the relationships between OT and other features
correlation_matrix = data.corr()
print(correlation_matrix['OT'].sort_values(ascending=False))

# Final dataset
print(data.head())

"""#Model Selection and Training"""

# Define features and target variable
X = data.drop(columns=['OT'])  # Drop the target variable
y = data['OT']  # Target variable

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

"""We will implement two models: Random Forest Regression and Gradient Boosting Regression. We will train each model and evaluate their performance.

A. Random Forest Regression
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score

# Initialize the Random Forest model
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# Fit the model
rf_model.fit(X_train, y_train)

# Predict on the test set
rf_predictions = rf_model.predict(X_test)

# Evaluate the model
rf_mse = mean_squared_error(y_test, rf_predictions)
rf_r2 = r2_score(y_test, rf_predictions)

print(f"Random Forest MSE: {rf_mse:.2f}")
print(f"Random Forest R2 Score: {rf_r2:.2f}")

"""B. Gradient Boosting Regression"""

from sklearn.ensemble import GradientBoostingRegressor

# Drop rows with NaN values
X_train_clean = X_train.dropna()
y_train_clean = y_train[X_train_clean.index]

X_test_clean = X_test.dropna()
y_test_clean = y_test[X_test_clean.index]

gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
# Train the model again
gb_model.fit(X_train_clean, y_train_clean)
gb_predictions = gb_model.predict(X_test_clean)

# Evaluate
gb_mse = mean_squared_error(y_test_clean, gb_predictions)
gb_r2 = r2_score(y_test_clean, gb_predictions)

print(f"Gradient Boosting MSE: {gb_mse:.2f}")
print(f"Gradient Boosting R2 Score: {gb_r2:.2f}")

# Print a comparison of both models
print("Model Comparison:")
print(f"Random Forest MSE: {rf_mse:.2f}, R2 Score: {rf_r2:.2f}")
print(f"Gradient Boosting MSE: {gb_mse:.2f}, R2 Score: {gb_r2:.2f}")

# Determine the better model
if rf_r2 > gb_r2:
    print("Random Forest performs better.")
else:
    print("Gradient Boosting performs better.")

import matplotlib.pyplot as plt

# Use the best model for predictions
predictions = rf_predictions if rf_r2 > gb_r2 else gb_predictions

# Plot actual vs predicted
plt.figure(figsize=(10, 5))
plt.plot(y_test.index, y_test, label='Actual', color='blue')
plt.plot(y_test.index, predictions, label='Predicted', color='orange')
plt.xlabel('Date')
plt.ylabel('Oil Temperature (OT)')
plt.title('Actual vs Predicted Oil Temperature')
plt.legend()
plt.show()

"""#Model evaluation and Result Analysis

A. Import Additional Libraries
"""

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import cross_val_score
import numpy as np

"""B. Add More Evaluation Metrics (MAE, RMSE)"""

# Additional evaluation metrics
rf_mae = mean_absolute_error(y_test, rf_predictions)
rf_rmse = np.sqrt(mean_squared_error(y_test, rf_predictions))

gb_mae = mean_absolute_error(y_test_clean, gb_predictions)
gb_rmse = np.sqrt(mean_squared_error(y_test_clean, gb_predictions))

# Print evaluation metrics for both models
print("Random Forest Evaluation:")
print(f"Mean Squared Error (MSE): {rf_mse:.2f}")
print(f"Mean Absolute Error (MAE): {rf_mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {rf_rmse:.2f}")
print(f"R2 Score: {rf_r2:.2f}")

print("\nGradient Boosting Evaluation:")
print(f"Mean Squared Error (MSE): {gb_mse:.2f}")
print(f"Mean Absolute Error (MAE): {gb_mae:.2f}")
print(f"Root Mean Squared Error (RMSE): {gb_rmse:.2f}")
print(f"R2 Score: {gb_r2:.2f}")

"""C. Implement Cross-Validation"""

# Perform 5-fold cross-validation for both models
rf_cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')
gb_cv_scores = cross_val_score(gb_model, X_train_clean, y_train_clean, cv=5, scoring='neg_mean_squared_error')

# Calculate the mean cross-validation score
rf_cv_mse = np.mean(-rf_cv_scores)
gb_cv_mse = np.mean(-gb_cv_scores)

print("\nCross-Validation Results:")
print(f"Random Forest CV MSE: {rf_cv_mse:.2f}")
print(f"Gradient Boosting CV MSE: {gb_cv_mse:.2f}")

"""D. Final Model Comparison Based on All Metrics"""

print("\nFinal Model Comparison:")
print(f"Random Forest - MSE: {rf_mse:.2f}, MAE: {rf_mae:.2f}, RMSE: {rf_rmse:.2f}, R2: {rf_r2:.2f}, CV MSE: {rf_cv_mse:.2f}")
print(f"Gradient Boosting - MSE: {gb_mse:.2f}, MAE: {gb_mae:.2f}, RMSE: {gb_rmse:.2f}, R2: {gb_r2:.2f}, CV MSE: {gb_cv_mse:.2f}")

if rf_cv_mse < gb_cv_mse:
    print("\nRandom Forest performs better on cross-validation.")
else:
    print("\nGradient Boosting performs better on cross-validation.")

"""E. Plot Improvement for Better Visualization"""

# Plot actual vs predicted with rolling mean for better visualization
plt.figure(figsize=(10, 5))
plt.plot(y_test.index, y_test.rolling(24).mean(), label='Actual (Rolling Mean)', color='blue')
plt.plot(y_test.index, pd.Series(predictions, index=y_test.index).rolling(24).mean(), label='Predicted (Rolling Mean)', color='orange')
plt.xlabel('Date')
plt.ylabel('Oil Temperature (OT)')
plt.title('Actual vs Predicted Oil Temperature (Smoothed)')
plt.legend()
plt.show()

"""#Exploring Improvements & Model Retraining

The visualization you have is cluttered, making it difficult to distinguish between the actual and predicted values clearly. Letâ€™s work on improving the visualization and model further.
"""

# Plot only a subset of the data to reduce clutter (here, first 1000 points)
subset_size = 1000
plt.figure(figsize=(10, 5))

plt.plot(y_test.index[:subset_size], y_test[:subset_size].rolling(48).mean(), label='Actual (Rolling Mean)', color='blue')
plt.plot(y_test.index[:subset_size], pd.Series(predictions[:subset_size], index=y_test.index[:subset_size]).rolling(48).mean(), label='Predicted (Rolling Mean)', color='orange')

plt.xlabel('Date')
plt.ylabel('Oil Temperature (OT)')
plt.title('Actual vs Predicted Oil Temperature (Subset and Smoothed)')
plt.legend()
plt.show()

"""Given that your MSE and RMSE are extremely low, there might be overfitting. This is indicated by the perfect R2 score = 1.00, which is uncommon in real-world data.

Add Regularization to the Models
"""

# Limit tree depth to reduce overfitting
rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)
rf_model.fit(X_train, y_train)


# Increase learning rate and reduce the number of trees
gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=4, random_state=42)
gb_model.fit(X_train_clean, y_train_clean)

"""Cross-Validation with More Robust Scoring"""

# Perform 5-fold cross-validation with MAE
rf_cv_mae = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='neg_mean_absolute_error')
gb_cv_mae = cross_val_score(gb_model, X_train_clean, y_train_clean, cv=5, scoring='neg_mean_absolute_error')

# Convert negative MAE to positive
rf_cv_mae_mean = np.mean(-rf_cv_mae)
gb_cv_mae_mean = np.mean(-gb_cv_mae)

print("\nCross-Validation MAE Results:")
print(f"Random Forest CV MAE: {rf_cv_mae_mean:.2f}")
print(f"Gradient Boosting CV MAE: {gb_cv_mae_mean:.2f}")

print(data.head())

"""#Trying Other Models

Applying LSTM for Multivariate Time Series Forecasting
"""

from sklearn.preprocessing import MinMaxScaler
import numpy as np
import pandas as pd

# Select features
features = ['HUFL', 'HULL', 'MUFL', 'MULL', 'LUFL', 'LULL', 'OT']

# Scale the data
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(data[features])

# Function to create dataset with look-back period
def create_lstm_dataset(data, time_step=24):
    X, y = [], []
    for i in range(len(data) - time_step - 1):
        X.append(data[i:(i + time_step), :-1])
        y.append(data[i + time_step, -1])
    return np.array(X), np.array(y)

# Prepare data for LSTM
time_step = 24  # Using past 24 hours to predict the next time step
X, y = create_lstm_dataset(scaled_data, time_step)


X = X.reshape(X.shape[0], X.shape[1], X.shape[2])

# Split into training and testing
train_size = int(len(X) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

# Build LSTM model
model = Sequential()
model.add(LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(LSTM(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))  # Predict the 'OT' value

# Compile the model
model.compile(optimizer='adam', loss='mean_squared_error')

# Train the model
model.fit(X_train, y_train, batch_size=64, epochs=10)

predictions = model.predict(X_test)

# Inverse transform the predictions and actual values
scaled_predictions = np.zeros((len(predictions), len(features)))
scaled_predictions[:, -1] = predictions[:, 0]  # Place predictions in the 'OT' column

# Inverse transform predictions back to original scale
actual_predictions = scaler.inverse_transform(scaled_predictions)[:, -1]

# Plot the actual vs predicted
plt.figure(figsize=(10, 5))
plt.plot(data.index[-len(y_test):], scaler.inverse_transform(scaled_data)[-len(y_test):, -1], label='Actual OT')
plt.plot(data.index[-len(y_test):], actual_predictions, label='Predicted OT', color='orange')
plt.title('LSTM Model: Actual vs Predicted Oil Temperature')
plt.xlabel('Date')
plt.ylabel('Oil Temperature')
plt.legend()
plt.show()

"""Applying ARIMA for Time Series Forecasting"""

from statsmodels.tsa.stattools import adfuller

# Check stationarity of OT
result = adfuller(data['OT'])
print(f'ADF Statistic: {result[0]}')
print(f'p-value: {result[1]}')

if result[1] > 0.05:
    print("The series is not stationary. Differencing is required.")
    data['OT_diff'] = data['OT'].diff().dropna()
else:
    print("The series is stationary.")

import warnings
import itertools
import numpy as np
import pandas as pd
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.stattools import adfuller

# Ignore warnings
warnings.filterwarnings("ignore")

# The grid search parameters
p = range(0, 4)
d = range(0, 2)
q = range(0, 4)

# Generate all different combinations of p, d, q triplets
pdq = list(itertools.product(p, d, q))

# Load or use the time series 'OT' data (univariate case for ARIMA)
ts_data = data['OT']

# Check if the series is stationary and difference if required
result = adfuller(ts_data.dropna())
if result[1] > 0.05:
    print("The series is not stationary. Applying differencing...")
    ts_data = ts_data.diff().dropna()  # First-order differencing (d=1)

# Function to evaluate and find the best parameters based on AIC
def grid_search_arima(ts_data, pdq):
    best_aic = np.inf
    best_order = None
    best_mdl = None

    for param in pdq:
        try:
            model = ARIMA(ts_data, order=param)
            results = model.fit()
            if results.aic < best_aic:
                best_aic = results.aic
                best_order = param
                best_mdl = results
            print(f"ARIMA{param} - AIC:{results.aic}")
        except Exception as e:
            print(f"ARIMA{param} failed. Error: {e}")
            continue
    return best_order, best_aic, best_mdl

# Perform grid search to find the best (p,d,q)
best_order, best_aic, best_mdl = grid_search_arima(ts_data, pdq)

# Print the best parameters and their corresponding AIC
print(f"\nBest ARIMA order: {best_order}")
print(f"Best AIC: {best_aic}")

from statsmodels.tsa.arima.model import ARIMA

# Fit ARIMA model (used p, d, q based on above analysis)
model = ARIMA(data['OT'], order=(1, 1, 2))
arima_result = model.fit()

# Summary of the model
print(arima_result.summary())

# Forecast next steps
forecast_steps = 100
forecast = arima_result.forecast(steps=forecast_steps)

# Get confidence intervals separately
conf_int = arima_result.get_forecast(steps=forecast_steps).conf_int()

# Create future index for plotting
future_index = pd.date_range(start=data.index[-1], periods=forecast_steps + 1, freq='H')[1:]

# Plot actual vs forecasted values
plt.figure(figsize=(10, 5))
plt.plot(data['OT'], label='Actual OT')
plt.plot(future_index, forecast, label='ARIMA Forecast', color='orange')
plt.fill_between(future_index, conf_int.iloc[:, 0], conf_int.iloc[:, 1], color='k', alpha=0.1)
plt.title('ARIMA Model: Actual vs Forecasted OT')
plt.xlabel('Date')
plt.ylabel('Oil Temperature')
plt.legend()
plt.show()